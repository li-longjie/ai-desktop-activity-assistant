# activity_retriever.py

import json
import os
import logging
from datetime import datetime, timedelta
import sqlite3
import re
from collections import defaultdict
from typing import List, Dict, Any, Optional

import chromadb
from chromadb.utils import embedding_functions
from dateparser.search import search_dates

# æ£€æŸ¥æ˜¯å¦åº”è¯¥åŠ è½½åµŒå…¥æ¨¡åž‹ï¼ˆå¯ä»¥é€šè¿‡çŽ¯å¢ƒå˜é‡æŽ§åˆ¶ï¼‰
LOAD_EMBEDDINGS = os.getenv('LOAD_EMBEDDINGS', 'true').lower() == 'true'

if LOAD_EMBEDDINGS:
    try:
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥æ¨¡å—ï¼ˆæ›´ç¨³å®šï¼‰
        from custom_embeddings import init_embeddings, search_similar, add_documents, clear_collection, get_collection_count, get_all_documents
        USE_LANGCHAIN = False
        logging.info("ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥æ¨¡å—")
    except ImportError:
        try:
            # é™çº§ä½¿ç”¨æ ‡å‡†LangChainï¼ˆPython 3.12+ï¼‰
            from langchain_huggingface import HuggingFaceEmbeddings
            from langchain_community.vectorstores import Chroma
            USE_LANGCHAIN = True
            logging.info("ä½¿ç”¨ langchain-huggingface (Python 3.12+)")
        except ImportError:
            USE_LANGCHAIN = False
            logging.error("æ— æ³•å¯¼å…¥ä»»ä½•åµŒå…¥æ¨¡å—")
else:
    USE_LANGCHAIN = False

from llm_service import LLMService

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- File Paths and Constants ---
# ä»Žé…ç½®æ–‡ä»¶è¯»å–è·¯å¾„
try:
    from gui_config import gui_config
    SCREENSHOT_DIR = gui_config.get('paths.screenshot_directory', 'screen_recordings')
    CHROMA_DB_PATH = gui_config.get('paths.database_directory', 'chroma_db_activity')
except ImportError:
    SCREENSHOT_DIR = "screen_recordings"
    CHROMA_DB_PATH = "chroma_db_activity"

DATABASE_FILE = os.path.join(SCREENSHOT_DIR, "activity_log.db")
COLLECTION_NAME = "screen_activity"

# --- Global Variables (used for initialization) ---
embeddings = None
activity_vector_store = None
collection = None

# --- Initialization ---
if LOAD_EMBEDDINGS:
    try:
        if USE_LANGCHAIN:
            # ä½¿ç”¨æ ‡å‡†LangChainï¼ˆPython 3.12+ï¼‰
            embeddings = HuggingFaceEmbeddings(
                model_name="Alibaba-NLP/gte-multilingual-base",
                model_kwargs={'device': 'cpu', 'trust_remote_code': True},
                encode_kwargs={'normalize_embeddings': True}
            )
            logging.info("åµŒå…¥æ¨¡åž‹åŠ è½½æˆåŠŸã€‚")

            activity_vector_store = Chroma(
                collection_name=COLLECTION_NAME,
                embedding_function=embeddings,
                persist_directory=CHROMA_DB_PATH
            )
            collection = activity_vector_store._collection
            logging.info(f"æˆåŠŸè¿žæŽ¥åˆ°ChromaDBå¹¶èŽ·å–/åˆ›å»ºé›†åˆ: {COLLECTION_NAME} at {CHROMA_DB_PATH}")
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥æ¨¡å—
            success = init_embeddings()
            if success:
                embeddings = True  # æ ‡è®°å·²åˆå§‹åŒ–
                activity_vector_store = True  # æ ‡è®°å·²åˆå§‹åŒ–
                collection = True  # æ ‡è®°å·²åˆå§‹åŒ–
                logging.info("è‡ªå®šä¹‰åµŒå…¥æ¨¡åž‹å’ŒChromaDBåˆå§‹åŒ–æˆåŠŸ")
            else:
                raise RuntimeError("è‡ªå®šä¹‰åµŒå…¥æ¨¡å—åˆå§‹åŒ–å¤±è´¥")

    except Exception as e:
        logging.error(f"åµŒå…¥æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        # Reset globals on failure
        embeddings = None
        activity_vector_store = None
        collection = None
else:
    logging.info("è·³è¿‡åµŒå…¥æ¨¡åž‹åŠ è½½ï¼ˆLOAD_EMBEDDINGS=falseï¼‰")

def create_db_connection():
    """Creates a connection to the SQLite database."""
    conn = None
    try:
        os.makedirs(SCREENSHOT_DIR, exist_ok=True)
        conn = sqlite3.connect(DATABASE_FILE)
        conn.row_factory = sqlite3.Row
    except sqlite3.Error as e:
        logging.error(f"è¿žæŽ¥SQLiteæ•°æ®åº“å¤±è´¥ ({DATABASE_FILE}): {e}")
    return conn

def load_and_index_activity_data() -> int:
    """Loads and indexes data from SQLite to ChromaDB on startup."""
    # æ£€æŸ¥æ˜¯å¦è·³è¿‡æ•°æ®ç´¢å¼•ï¼ˆç”¨äºŽå¿«é€Ÿå¯åŠ¨ï¼‰
    SKIP_INDEXING = os.getenv('SKIP_INDEXING', 'false').lower() == 'true'
    if SKIP_INDEXING:
        logging.info("â© è·³è¿‡æ•°æ®ç´¢å¼•ï¼ˆSKIP_INDEXING=trueï¼‰ï¼Œä½¿ç”¨çŽ°æœ‰æ•°æ®")
        return 0
    
    if not LOAD_EMBEDDINGS or collection is None or embeddings is None:
        logging.error("ChromaDB collection or embeddings not initialized. Cannot index data.")
        return 0

    conn = create_db_connection()
    if not conn:
        return 0

    new_records_count = 0
    try:
        # æ£€æŸ¥çŽ°æœ‰æ•°æ®
        if USE_LANGCHAIN:
            existing_count = len(collection.get(include=[])['ids'])
            existing_ids_set = set(collection.get(include=[])['ids'])
        else:
            existing_count = get_collection_count()
            existing_data = get_all_documents()
            existing_ids_set = set(doc.get('id', '') for doc in existing_data) if existing_data else set()
        
        # å¼ºåˆ¶é‡æ–°ç´¢å¼•çš„æƒ…å†µ
        FORCE_REINDEX = os.getenv('FORCE_REINDEX', 'false').lower() == 'true'
        if FORCE_REINDEX:
            logging.info("ðŸ”„ å¼ºåˆ¶é‡æ–°ç´¢å¼•æ‰€æœ‰æ•°æ®...")
            
            if USE_LANGCHAIN:
                existing_ids = collection.get(include=[])['ids']
                if existing_ids:
                    collection.delete(ids=existing_ids)
            else:
                clear_collection()
            existing_ids_set = set()

        # ä»ŽSQLiteåŠ è½½æ‰€æœ‰è®°å½•
        cursor = conn.cursor()
        cursor.execute("SELECT id, timestamp, ocr_text, app_name, window_title, screenshot_path FROM activity_log WHERE ocr_text IS NOT NULL AND ocr_text != ''")
        records = cursor.fetchall()

        if not records:
            return 0

        # æ‰¾å‡ºæ–°è®°å½•
        new_documents = []
        new_metadatas = []
        new_ids = []
        
        for row in records:
            record = dict(row)
            record_id = f"activity_{record['id']}"
            
            # åªå¤„ç†æ–°è®°å½•
            if record_id not in existing_ids_set:
                doc_text = f"åº”ç”¨: {record.get('app_name')} | çª—å£: {record.get('window_title')} | å†…å®¹: {record.get('ocr_text')}"
                new_documents.append(doc_text)

                metadata = {k: str(v) for k, v in record.items() if v is not None}
                try:
                    if 'timestamp' in metadata:
                        metadata['timestamp'] = datetime.fromisoformat(record['timestamp']).timestamp()
                except (TypeError, ValueError):
                    logging.warning(f"Could not parse timestamp for record {record.get('id')}.")
                    if 'timestamp' in metadata:
                        del metadata['timestamp']
                
                new_metadatas.append(metadata)
                new_ids.append(record_id)

        # åªæœ‰æ–°è®°å½•æ—¶æ‰æ·»åŠ åˆ°ChromaDB
        if new_documents:
            if USE_LANGCHAIN:
                collection.add(documents=new_documents, metadatas=new_metadatas, ids=new_ids)
            else:
                add_documents(documents=new_documents, metadatas=new_metadatas, ids=new_ids)
            new_records_count = len(new_documents)

    except Exception as e:
        logging.error(f"Error during data indexing: {e}", exc_info=True)
    finally:
        if conn:
            conn.close()
    
    return new_records_count

def parse_time_range_from_query(query_text: str) -> tuple[datetime, datetime]:
    """Parses a time range from the user's query."""
    now = datetime.now()
    start_time, end_time = now - timedelta(days=1), now

    # ä¼˜å…ˆå¤„ç†å¸¸è§çš„ä¸­æ–‡æ—¶é—´è¡¨è¾¾
    if "ä»Šå¤©" in query_text:
        start_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time = now
        logging.info(f"è§£æž'ä»Šå¤©': {start_time} åˆ° {end_time}")
        return start_time, end_time
    
    if "æ˜¨å¤©" in query_text:
        yesterday = now - timedelta(days=1)
        start_time = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
        logging.info(f"è§£æž'æ˜¨å¤©': {start_time} åˆ° {end_time}")
        return start_time, end_time

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç† "æœ€è¿‘Xåˆ†é’Ÿ/å°æ—¶/å¤©" æˆ– "è¿‡åŽ»Xåˆ†é’Ÿ/å°æ—¶/å¤©"
    # æ”¯æŒæ•°å­—å’Œéƒ¨åˆ†ä¸­æ–‡æ•°å­—ï¼ˆä¸€è‡³åï¼‰
    num_map_chinese_to_int = {"ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5, "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10}
    # åŒ¹é…ä¾‹å¦‚: "è¿‡åŽ»5åˆ†é’Ÿ", "æœ€è¿‘ä¸€å°æ—¶", "è¿‡åŽ»åå¤©", "æœ€è¿‘äº”åˆ†é’Ÿ"
    match_duration = re.search(r"(?:æœ€è¿‘|è¿‡åŽ»)([\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*(åˆ†é’Ÿ|å°æ—¶|å¤©|å‘¨|æœˆ)", query_text)

    if match_duration:
        value_str = match_duration.group(1)
        unit = match_duration.group(2)
        value = 0

        if value_str in num_map_chinese_to_int:
            value = num_map_chinese_to_int[value_str]
        else:
            try:
                value = int(value_str)
            except ValueError:
                logging.warning(f"æ— æ³•ä»Ž '{value_str}' è§£æžæ•°å­—ç”¨äºŽæ—¶é—´èŒƒå›´æŸ¥è¯¢ã€‚")
                value = 0 
        
        if value > 0:
            if unit == "åˆ†é’Ÿ":
                start_time = now - timedelta(minutes=value)
            elif unit == "å°æ—¶":
                start_time = now - timedelta(hours=value)
            elif unit == "å¤©":
                start_time = now - timedelta(days=value)
            elif unit == "å‘¨":
                start_time = now - timedelta(weeks=value)
            elif unit == "æœˆ": # è¿‘ä¼¼æœˆä»½
                start_time = now - timedelta(days=value * 30)
            end_time = now
            logging.info(f"é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è§£æžæ—¶é—´: å€¼={value}, å•ä½='{unit}'. è®¡ç®—æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
    else:
        # å¦‚æžœæ²¡æœ‰ç‰¹å®šçš„æ—¶é—´è¡¨è¾¾å¼ï¼Œé»˜è®¤ä½¿ç”¨è¿‡åŽ»24å°æ—¶
        start_time = now - timedelta(days=1)
        end_time = now
        logging.info(f"ä½¿ç”¨é»˜è®¤æ—¶é—´èŒƒå›´(è¿‡åŽ»24å°æ—¶): {start_time} åˆ° {end_time}")
    
    return start_time, end_time

# --- Main Class for Activity Retrieval ---
class ActivityRetriever:
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        # Check if embeddings are initialized
        if not LOAD_EMBEDDINGS or embeddings is None:
            raise RuntimeError("ActivityRetriever cannot be initialized because the embeddings failed to load.")

    async def retrieve_and_answer(self, user_query: str) -> tuple[str, list]:
        """
        Retrieves relevant activities based on a user query and uses an LLM to answer.
        This is the core logic method.
        """
        start_time_dt, end_time_dt = parse_time_range_from_query(user_query)
        logging.info(f"Searching for activities between {start_time_dt} and {end_time_dt}")

        try:
            if USE_LANGCHAIN:
                # ä½¿ç”¨LangChainæ¨¡å¼æœç´¢
                filter_dict = {
                    "$and": [
                        {"timestamp": {"$gte": start_time_dt.timestamp()}},
                        {"timestamp": {"$lte": end_time_dt.timestamp()}}
                    ]
                }
                
                search_results = activity_vector_store.similarity_search_with_score(
                    query=user_query,
                    k=25,
                    filter=filter_dict
                )
                
                docs = [doc for doc, score in search_results]
            else:
                # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å—æœç´¢
                where_filter = {
                    "$and": [
                        {"timestamp": {"$gte": start_time_dt.timestamp()}},
                        {"timestamp": {"$lte": end_time_dt.timestamp()}}
                    ]
                }
                
                results = search_similar(query=user_query, k=25, where_filter=where_filter)
                
                # Convert to document format
                docs = []
                for result in results:
                    doc = type('Document', (), {
                        'page_content': result['page_content'],
                        'metadata': result['metadata']
                    })()
                    docs.append(doc)

        except Exception as e:
            logging.error(f"Error during similarity search: {e}", exc_info=True)
            return "Failed to search for activities in the vector database.", []
        
        if not docs:
            return f"åœ¨æ—¶é—´èŒƒå›´ {start_time_dt.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M')} å†…æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ´»åŠ¨è®°å½•ã€‚", []

        screenshots = [doc.metadata.get('screenshot_path') for doc in docs if doc.metadata.get('screenshot_path')]
        
        context_parts = []
        for doc in docs:
            ts = doc.metadata.get('timestamp')
            # Format timestamp back to a readable string if it's a number
            try:
                readable_ts = datetime.fromtimestamp(float(ts)).strftime('%Y-%m-%d %H:%M:%S')
            except (ValueError, TypeError):
                readable_ts = str(ts) # Fallback to original string if conversion fails
            
            context_parts.append(f"### {readable_ts}\n{doc.page_content}")

        context = "\n\n".join(context_parts)

        if not context.strip():
            return f"åœ¨æ—¶é—´èŒƒå›´ {start_time_dt.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M')} å†…æ‰¾åˆ°äº†æ´»åŠ¨ï¼Œä½†æ²¡æœ‰è¯†åˆ«å‡ºçš„æ–‡æœ¬å†…å®¹ã€‚", screenshots

        prompt_template = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¸ªäººåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºŽä»¥ä¸‹ç”± OmniParser ç”Ÿæˆçš„å±å¹•æ´»åŠ¨æ—¥å¿—æ¥å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¿™äº›æ—¥å¿—åŒ…å«äº†é€šè¿‡è§†è§‰åˆ†æžå±å¹•è¯†åˆ«å‡ºçš„UIå…ƒç´ ã€æ–‡æœ¬å’Œå›¾æ ‡ã€‚è¯·åˆ©ç”¨è¿™äº›è¯¦ç»†ä¿¡æ¯æ¥æä¾›ç²¾å‡†çš„å›žç­”ã€‚

---
ã€å±å¹•æ´»åŠ¨æ—¥å¿—ã€‘
{context}
---

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}
"""
        final_prompt = prompt_template.format(context=context, query=user_query)

        if self.llm_service:
            try:
                response = await self.llm_service.get_response(final_prompt)
                return response, screenshots
            except Exception as e:
                logging.warning(f"LLMæœåŠ¡è°ƒç”¨å¤±è´¥: {e}ï¼Œå°†è¿”å›žåŽŸå§‹æ•°æ®æ‘˜è¦")
                # å¦‚æžœLLMå¤±è´¥ï¼Œè¿”å›žç®€å•çš„æ•°æ®æ‘˜è¦
                return self._generate_simple_summary(docs, start_time_dt, end_time_dt), screenshots
        else:
            return self._generate_simple_summary(docs, start_time_dt, end_time_dt), screenshots
    
    def _generate_simple_summary(self, docs, start_time_dt, end_time_dt) -> str:
        """ç”Ÿæˆç®€å•çš„æ•°æ®æ‘˜è¦ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        if not docs:
            return f"åœ¨æ—¶é—´èŒƒå›´ {start_time_dt.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M')} å†…æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•ã€‚"
        
        # ç»Ÿè®¡åº”ç”¨ä½¿ç”¨æƒ…å†µ
        apps = {}
        timestamps = []
        
        for doc in docs:
            metadata = doc.metadata
            app_name = metadata.get('app_name', 'Unknown')
            timestamp = metadata.get('timestamp', 0)
            
            if app_name != 'Unknown':
                apps[app_name] = apps.get(app_name, 0) + 1
            
            try:
                timestamps.append(float(timestamp))
            except (ValueError, TypeError):
                pass
        
        # ç”Ÿæˆæ‘˜è¦
        time_range = f"{start_time_dt.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time_dt.strftime('%Y-%m-%d %H:%M')}"
        summary_parts = [f"ðŸ“… æ—¶é—´èŒƒå›´: {time_range}", f"ðŸ“Š æ‰¾åˆ° {len(docs)} æ¡æ´»åŠ¨è®°å½•"]
        
        if apps:
            summary_parts.append("ðŸŽ¯ ä¸»è¦åº”ç”¨:")
            for app, count in sorted(apps.items(), key=lambda x: x[1], reverse=True):
                summary_parts.append(f"   â€¢ {app}: {count} æ¬¡è®°å½•")
        
        if timestamps:
            timestamps.sort(reverse=True)
            latest_time = datetime.fromtimestamp(timestamps[0]).strftime('%H:%M:%S')
            earliest_time = datetime.fromtimestamp(timestamps[-1]).strftime('%H:%M:%S')
            summary_parts.append(f"â° æ´»åŠ¨æ—¶é—´: {earliest_time} - {latest_time}")
        
        return "\n".join(summary_parts)

# --- Standalone Functions for API Endpoints ---
def get_all_activity_records(limit: int = 50) -> list:
    """Retrieves all activity records from the SQLite database."""
    conn = create_db_connection()
    if not conn: return []
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM activity_log ORDER BY timestamp DESC LIMIT ?", (limit,))
        return [dict(row) for row in cursor.fetchall()]
    finally:
        if conn: conn.close()

async def get_application_usage_summary(start_time_dt: datetime, end_time_dt: datetime) -> dict:
    """Calculates application usage summary within a given time range."""
    conn = create_db_connection()
    if not conn: return {"error": "Database connection failed."}

    summary = defaultdict(timedelta)
    try:
        cursor = conn.cursor()
        cursor.execute(
            "SELECT timestamp, app_name FROM activity_log WHERE timestamp BETWEEN ? AND ? AND app_name IS NOT NULL ORDER BY timestamp ASC",
            (start_time_dt.isoformat(), end_time_dt.isoformat())
        )
        events = [dict(row) for row in cursor.fetchall()]

        if not events: return {"usage": {}}

        for i, event in enumerate(events):
            start = datetime.fromisoformat(event['timestamp'])
            end = datetime.fromisoformat(events[i+1]['timestamp']) if i + 1 < len(events) else end_time_dt
            duration = end - start
            if duration.total_seconds() > 0:
                summary[event['app_name']] += duration
        
        return {"usage": dict(summary)}
    except Exception as e:
        return {"error": str(e)}
    finally:
        if conn: conn.close()